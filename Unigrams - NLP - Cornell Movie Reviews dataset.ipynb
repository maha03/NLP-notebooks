{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This notebook contains code implementing NLP on the Cornell Movie Review dataset v2.0 (polarity dataset v2.0) found at https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "The dataset contains 1000 positive and 1000 negative processed reviews in down-cased text files.\n",
    "\n",
    "Step 1: Implementing nltk libraries \n",
    "a. preprocess the document -removing special characters,single characters,multiple spaces\n",
    "b. Tokenize\n",
    "c. Lemmatize\n",
    "\n",
    "Step 2: Implementing Bag of Words model to convert text into numbers. \n",
    "a. uses 15000 features and bigrams \n",
    "b. Term-Frequency Inverse Document Frequency is used\n",
    "\n",
    "Step 3: Building classifiers using Linear SVM & Random Forest algorithms\n",
    "a. Hyper-tuning is also performed using GridSearchCV\n",
    "b. Training accuracy & Testing accuracy is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files #To load text files with categories as subfolder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Review files\n",
    "movie_data = load_files(r\"C:\\Users\\mahaa\\Desktop\\review_polarity.tar\\txt_sentoken\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mahaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Loading nltk libraries and downloading nltk data\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process the review files and store in the variable 'documents'\n",
    "documents=[]\n",
    "for doc in range(0,len(X)):\n",
    "    document = re.sub(r'\\W', ' ', str(X[doc]))\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    word_tokens=word_tokenize(document)\n",
    "    document=[lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Bag of Words model with Unigrams using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max_features=15000 was approximated from the research paper published by the data owner at \n",
    "https://www.cs.cornell.edu/home/llee/papers/sentiment.pdf'''\n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = CountVectorizer(max_features=15000,max_df=0.5,stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_as_array = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply TF-IDF transform\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()\n",
    "documents_as_array_tfidf= tfidfconverter.fit_transform(documents_as_array).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Machine Learning classifier using Linear SVM, Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting train and test data with 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents_as_array_tfidf, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM classifier\n",
    "lsvm_classifier=LinearSVC(random_state=0,penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvm_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm_y_train_pred=lsvm_classifier.predict(X_train)\n",
    "lsvm_y_test_pred = lsvm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Testing accuracy 0.8225\n",
      "[[163  45]\n",
      " [ 26 166]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy\",accuracy_score(y_train, lsvm_y_train_pred))\n",
    "print(\"Testing accuracy\",accuracy_score(y_test, lsvm_y_test_pred))\n",
    "print(confusion_matrix(y_test,lsvm_y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM classifier with hypertuned parameters\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000]}\n",
    "lsvm_grid = GridSearchCV(LinearSVC(),param_grid,cv=10,verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\mahaa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   50.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LinearSVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000]}, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvm_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm_grid_ytrain_pred=lsvm_grid.predict(X_train)\n",
    "lsvm_grid_ytest_pred=lsvm_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "print(lsvm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy-Grid search 1.0\n",
      "Testing accuracy-Grid search 0.8225\n",
      "[[163  45]\n",
      " [ 26 166]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy-Grid search\",accuracy_score(y_train, lsvm_grid_ytrain_pred))\n",
    "print(\"Testing accuracy-Grid search\",accuracy_score(y_test, lsvm_grid_ytest_pred))\n",
    "print(confusion_matrix(y_test,lsvm_grid_ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier\n",
    "rndf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rndf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf_y_train_pred=rndf.predict(X_train)\n",
    "rndf_y_test_pred =rndf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Testing accuracy 0.815\n",
      "[[177  31]\n",
      " [ 43 149]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy\",accuracy_score(y_train, rndf_y_train_pred))\n",
    "print(\"Testing accuracy\",accuracy_score(y_test, rndf_y_test_pred))\n",
    "print(confusion_matrix(y_test, rndf_y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier with hyper tuned parameters\n",
    "rndf = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf_grid= GridSearchCV(estimator = rndf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 20.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 90, 100, 110],\n",
       "                         'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5],\n",
       "                         'min_samples_split': [8, 10, 12],\n",
       "                         'n_estimators': [100, 200, 300, 1000]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rndf_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 1000}\n",
      "RandomForestClassifier(max_depth=110, max_features=3, min_samples_leaf=3,\n",
      "                       min_samples_split=12, n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "print(rndf_grid.best_params_)\n",
    "print(rndf_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf_grid_ytrain_pred=rndf_grid.predict(X_train)\n",
    "rndf_grid_ytest_pred=rndf_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy-Grid search 0.971875\n",
      "Testing accuracy-Grid search 0.7125\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy-Grid search\",accuracy_score(y_train, rndf_grid_ytrain_pred))\n",
    "print(\"Testing accuracy-Grid search\",accuracy_score(y_test, rndf_grid_ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139  69]\n",
      " [ 46 146]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, rndf_grid_ytest_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                               Train accuracy    Test accuracy\n",
    "Linear SVM                       100                 82.25\n",
    "Linear SVM(GridSearchCV)         100                 82.25\n",
    "Random Forest                    100                 81.5   \n",
    "Random Forest(GridSearchCV)      97.18               71.25\n",
    "\n",
    "It looks to me all the models are over-fitted; The False Positive being higher than False Negative in the models with hypertuned parameters is concerning. The hyper parameter tuning is not really necessary for linear SVM. Is it because I've missed soemthing?\n",
    "\n",
    "I tried creating Bag of Word Models with Bigrams; The accuracies were as below:\n",
    "\n",
    "                               Train accuracy    Test accuracy\n",
    "Linear SVM                       100                 79.75\n",
    "Linear SVM(GridSearchCV)         100                 79.75\n",
    "Random Forest                    100                 77.25  \n",
    "Random Forest(GridSearchCV)      91.9                61.5\n",
    "\n",
    "I think I should make the Vectorizer more robust and remove the years/numeric during pre-processing; In addition to the numbers, I'm  not sure if the Vectorizer_feature_names() is a good feature set; I can try Lemmatizing by also passing the Parts-Of-Speech(POS) to get context relevant features. \n",
    "\n",
    "But choosing the type of POS, choosing stemmer vs lemmatizer looks like a hard call for now.\n",
    "\n",
    "In addition, I'm wondering how to check if lemmatization results makes sense to our goal.\n",
    "\n",
    "This was the first time I came across sklearn.utils.Bunch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
